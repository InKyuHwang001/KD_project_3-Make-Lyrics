{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "글로브에대한 이해.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 글로브\n",
        "## 등장배경\n",
        "### 워드 임베딩의 두가지 방법\n",
        "- 말뭉치 전체의 통계적 정보를 활용 LSA\n",
        "- 신경망 등을 활용하여 저차원 벡터공간에 임베딩\n",
        "\n",
        "### 문제의 인식\n",
        "- LSA 등의 통계적 방식: \n",
        "  - 장점 : 말뭉치의 통계적 정보를 활용\n",
        "  - 단점 : 단어\\/문서 간의 유사도 측정이 난해\n",
        "- 워드2벡 등의 임베딩 방식:\n",
        "  - 장점: 유의어와 같은 단어의 유사도 기반의 TASK가 가능\n",
        "  - 단점: 주변의 일부 단어와 연관성만 학습하므로, 말뭉치 전체의 정보를 반영하지 못함\n",
        "### 해결방안\n",
        "  - Vector Embedding시 말뭉치 전체의 통계를 반영하자!\n",
        "\n",
        "## 원리\n",
        "- 임베딩 벡터간의 inner product가 말뭉치 전체에 대한 Co-occurrence 확률의 비율과 같도록 mapping함수 설계\n",
        "  - inner product: 두 벡터의 유사도 \n",
        "  - Co-occurrence: 빈도수에 관한 확률\n",
        "- https://wikidocs.net/22885\n",
        "- https://www.youtube.com/watch?v=uZ2GtEe-50E\n"
      ],
      "metadata": {
        "id": "EzH5YGr-rNfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyrsXECsrKs4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}