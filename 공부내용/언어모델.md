## 언어모델

> **언어모델이란?**
> : 단어 시퀀스(단어가 연속적으로 연결= 문장)에서 각 단어의 확률을 할당하는 일을 하는 모델
> : 즉, 가장 자연스러운 단어 시퀀스를 찾아내는 모델
>
> - 크게 **통계를 이용한 방법 / 인공신경망을 이용한 방법**으로 나뉨
> - **가장 보편적 예시**는 **이전 단어가 주어졌을 때 다음 단어를 예측**하도록 하여 단어 시퀀스에 확률을 할당
> - **Word2Vec**(CBow)은 여러 언어 모델 중 하나

### 통계적 언어 모델(SLM)

: 언어모델의 전통적인 접근 방법으로 줄여서 SLM이라고 함

- **단어의 등장 횟수**를 바탕으로 **조건부 확률** 계산

예시 ) "I am a student"
첫 번째 단어 "I"를 예시로 들 때,

\1) 전체 말뭉치 중 **"I"로 시작하는 문장의 횟수**를 구함
**2) 전체 말뭉치 1000개에 I로 시작하는 문장이 100개라면 0.1의 확률**
**P("I")** = I 문장개수 / 전체 문장 개수 = 100/1000 = 0.1

**3) "I" 다음 단어가 "am"이 나오는 문장이 50개라면**
**P("am"|"I")** = I am 문장 개수/ I 문장개수 = 50/100 = 0.5

![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2Fa01fe6b4-883f-4b4d-801a-f61190f55301%2Fimage.png)

**이렇게 구한 조건부 확률을 모두 곱하면 문장이 등장할 확률을 구할 수 있음**



 **통계적 언어모델의 한계**

>  **희소성 문제**
>
> 학습시키는 말뭉치에 word1, word2 표현은 있지만 word3표현이 없다면?
> ---> word3이 들어간 문장은 절대 만들 수 없음
>
> **실제로 word3가 사용되는 표현이어도 말뭉치에 포함되지 않은 이유로 사용할 수 없는 것이 희소성 문제이다**
>
> - **해결방법** : N-gram, Smoothing, Back-off



------

### 신경망 언어 모델(NLM)

- 횟수가 기반인 통계적 언어 모델과 달리 Word2Vec 이나 fastText 등의 출력값인 **임베딩 벡터**를 사용
- 통계적 언어 모델의 한계를 극복 --- > **말뭉치에 없어는 단어라도 의미적, 문법적 유사하다면 선택 가능**

예를 들어, word3가 말뭉치에 없을 때, word1, word2 등의 유사한 단어와 비슷한 곳에 위치하여 언어모델은 word3의 문장도 형성 가능함



## 순환 신경망(Recurrent neural network, RNN)

: 인공 신경망의 언어 모델(NLM)에서 사용됨
: 연속형 데이터를 잘 처리하기 위해 고안된 신경망

> **연속형 데이터란 ?**
>
> - 어떤 순서로 오냐에 따라 의미가 달라지는 데이터
> - 비연속형 데이터 : 데이터프레임, 고양이
> - 연속형 데이터 : 문장, 시계열 그래프

### RNN의 구조

- \1) 입력 벡터(각 단어들)가 은닉층으로 들어감 -- **X(t)**
- \2) 은닉층에서 출력벡터가 생성 -- **O(t)**
- \3) 은닉층에서 나와 다시 은닉층으로 입력됨 : 특정 시점의 은닉 벡터가 입력벡터로 다시 들어감 (기존 신경망에는 없는 구조) -- **h(t)가 t+1시점으로 들어감**

![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2Fcffd7aef-dd83-460e-9e23-fba390c38853%2Fimage.png)출처 : https://www.youtube.com/watch?v=PahF2hZM6cs
위 출처 영상에 들어가면 자세한 설명 볼 수 있다



**<RNN의 순서>**

1. t-1 시점에서 X(t-1)와 h(t-2)가 입력되고 O(t-1)이 출력
2. t 시점에선 X(t)와 h(t-1)가 입력되고 O(t)가 출력
3. t+1시점에선 X(t+1)과 h(t)가 입력되고 O(t+1)가 출력

![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2Fb577c2ed-32e2-418b-99ad-656786e1fe6d%2Fimage.png)

> 🔑 t시점의 RNN계층 출력값
> **h(t) = tahn( h(t-1)\*W(h) + X(t)\*W(x) + b )**





**<RNN : 출력 값이 이어지는 구조>**
![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2F8e696e3d-99f7-4e48-a0ba-9141cc7678c5%2Fimage.png)

**<RNN 원리 코드 구현>**

```python
import numpy as np

class RNN:
	def __init__(self, Wx, Wh, b):
   	self.params = [Wx, Wh, b]
       # 필요한 가중치 초기화 (가중치,bias 벡터 크기만큼 0으로 채워짐)
       self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)
     	self.cache = None
    
    # t시점
    def forward(self, x, h_prev): #이전 은닉층 출력값
    	Wx, Wh, b = self.params
       t = np.matul(h_prev, Wh) #이전은닉층과 은닉층 가중치 내적 
       h_next = np.tahn(t) #다음으로 넘어갈 h(t)
       
       self.cache = (x, h_prev, h_next)
		return h_next
```





### 다양한 형태의 RNN

![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2Fec11645e-7906-44c3-b66c-f3f273ecdcc2%2Fimage.png)

- **one to one** : 순환이 적용되지 않아 RNN으로 적합하지 않음

- **one to many** : 하나의 이미지를 받아 설명하는 문장 내보냄, ex)**이미지 캡셔닝**

- **many to one** : 하나의 문장을 받아 결론을 지음, ex) **해당 문장이 긍정인지 부정인지**

- **many to many 1** : Sequential 벡터를 입력받아 Sequential 벡터로 출력, ex) **번역할 문장을 받아 다시 번역된 문장을 내놓음(기계번역)**

- **many to many 2** : squential 벡터 입력받는 즉시 squential 벡터 바로 출력, ex) **프레임 단위의 비디오 분류**

  

### RNN의 장점과 단점

**<장점>**

- 모델이 간단하다

- 어떤 길이의 **연속형(sequential) 데이터**라도 처리가 가능

- 입력시퀀스와 출력시퀀스의 길이를 다르게 출력 가능
  ++ 시퀀스 : 순서가 있는 데이터

  

**<단점>**

- **병렬화가 불가능하다**
  : 벡터가 x1, x2, x3.. 로 순차적으로 입력되므로 연속형 처리에는 유용하나,
  GPU연산의 장점인 병렬화 이용이 불가능하다
  (RNN 모델이 GPU 연산을 이용하게 되면 장점은 없다)

  

- 기울기 폭발(Exploding Gradient) or 기울기 소실(Vanishing Gradient)

  의 가능성

  - **기울기 폭발 (Exploding Gradient)** :
    역전파 과정에서 값을 전달하며 반복해서 곱하는 과정이 있는데 그 값이 1보다 크다면 지수적으로 값이 커져 hidden state 벡터에 정보가 과하게 전달

  - **기울기 소실 (Vanishing Gradient)** :
    역전파 과정에서 활성화 함수인 **tahn의 미분값**을 전달하며 반복해서 곱해주는 과정이 많다. Tahn 함수의 미분값이 1이 최댓값이므로 **순환(Recurrent)으로 계속 곱할수록 값이 작아져 hidden state에 벡터 역전파 정보가 소실될 가능성**이 있음

    

- **장기 의존성 문제**
  : 긴 문장을 처리할 때 순차적으로 처리하므로 앞쪽에 입력된 단어를 잊어버릴 수 있음

  

**✨ 기울기 정보의 크기를 적절히 조정할 방법이 없을까 ?✨**

여기서 나온 해결 방법이 **"LSTM"**



## LSTM

### **LSTM (Long Term Short Memory, 장단기기억망)**

: **RNN의 기울기 정보 크기를 조절**하기 위한 **gate 추가**한 모델

- 기울기 소실 문제 해결
- 요즘은 대부분 LSTM 사용(RNN이라고 하면 LSTM이나 GRU를 말함)
- Sequential 데이터 처리에 대표적인 모델
- 기존 RNN을 오히려 RNN(Vanilla RNN)라고 구별지어 부름
- 데이터 모두 입력된 후 출력 벡터 생성(입력 벡터 개수와 출력벡터 개수 다를 수 있음)

### **LSTM 구조**

![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2F109afc8c-d6ef-4bd0-8c70-13ee3c3ce366%2Fimage.png)



**<LSTM의 세가지 게이트>**

**그림보면서 이해해보기 !**

- **input gate(\*i\**t\*)**
  ![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2Fcd61ca13-f52a-4dd6-82dc-22a045d4d94f%2Fimage.png)
  \- **새로 입력된 정보를 얼마나 장기상태에 활용할 것인가?**
  \- 현재(t) 시점의 X값과 입력 게이트로 이어지는 가중치 를 곱한 값에 (t-1)의 h(은닉상태)가 입력 게이트로 이어지는 가중치 를 곱한 값을 더해 **시그모이드 함수와 탄젠트 함수**를 적용
  \- 시그모이드(0~1 출력), tahn(-1~1 출력)의 값 두 개로 새로운 정보의 기억할 양을 정함

  

- **forget gate(\*f\**t\*)**

![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2F9d6af1f1-9ac8-4017-9cc6-ad836eb86558%2Fimage.png)

\- **과거 정보를 얼마나 유지할 것인가?**
\- 현재(t)시점의 X 값과 t-1 시점의 h 상태가 **시그모이드 함수**를 지남
(함수를 거쳐 나온 0과 1사이의 값은 삭제 과정을 지난 정보의 양)
\- 0에 가까울수록 많이 삭제, 1에 가까울수록 온전히 기억



- **output gate(\*o\**t\*)**

  ![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2F9742b2d5-476d-419e-9bb8-0d203377fc94%2Fimage.png)
  \- 출력 정보을 얼마나 넘겨줄 것인가?
  \- 시그모이드 함수를 지나 현재(t)시점의 은닉 상태(단기 상태,h)를 결정

++ **셀 상태(장기 상태)**
![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2Fbb65293f-8cbb-40d8-ae98-3546910afa23%2Fimage.png)

***i\**t\* \* \*g\**t\* : 현재 시점의 기억할 값**

*f**t** *C**t*−1 : **과거 정보에서 기억할 값**

현재 시점 기억할 값 + 과거 정보에서 기억할 값 = *C**t*





기존 hidden-state[h(t-1)] 에 **활성화 함수를 직접 거치지 않는 cell-state 추가[C(t-1)]**

**⭐ LSTM의 이러한 구조는 뒷쪽 시퀀스 정보에 비중을 결정 가능하며, 동시에 앞쪽 시퀀스의 정보를 완전히 잃지 않아 RNN의 장기 의존성 문제를 해결**

### **LSTM의 사용**

- 여러 언어 모델
- 기존 RNN(Vanilla RNN)의 경우 10-20 문장에 대한 처리 성능이 매우 낮아
  **신경망을 활용한 대부분 시계열 알고리즘에는 LSTM 사용**

### **LSTM의 코드**

```python
#텍스트 감정분류 
# keras를 이용해RNN/LSTM 사용해 감정분류 수행하는 tensorflow 코드

from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding
from tensorflow.keras.layers import LSTM
from tensorflow.keras.datasets import imdb


max_features= 20000  # 단어 집합 크기 제한
maxlen = 80 # 리뷰길이 제한
batch_size = 32

# 데이터 불러오기

(x_train,y_train ), (x_test, y_test) = imdb.load_data(num_words=max_features) 
 # num_words : 데이터에서 등장 빈도 순위로 몇 번째에 해당하는 단어까지를 사용할 것인지(단어 집합 크기)
 # x_train데이터는  토큰화와 정수 인코딩이 끝난 상태이며, 전체 데이터에서 각 단어들의 등장 빈도에 따라서 인덱스를 부여
 
     
# ---      
# padding : 내부 길이 맞춰주기
x_train = sequence.pad_sequences(x_train, maxlen=maxien) # maxlen : 문장 길이 제한,초과분 삭제

x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
     
     
     
# ---     
# 모델 정의 : LSTM 적용

import tensorflow as tf
model = tf.keras.models.Sequential([
                                tf.keras.layers.Embedding(max_features, 128), #입력층: max_features 단어 개수
                                tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2), #recurrent_dropout:순환 드롭아웃
                                tf.keras.layers.Dense(1, activation='sigmoid')  
                            
])
# Dropout / Recurrent Dropout 의 차이
# Dropout : 입력할 때 드롭할 비율
# Recurrent Dropout : 순환할 때 드롭할 비율

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])     
     
     
     
model.fit(x_train, y_train,
          batch_size=batch_size, 
          epochs=3, 
          validation_data=(x_test,y_test))     

     
# 받은 리뷰를 토큰화해서 빈도수에 따라 인코딩해서 80차원을 만들어 model에 넣어주면
# 리뷰가 긍정인지 부정인지 accuracy: % 로 알 수 있음
     
```

### 🔥 **GRU (Gated Recurrent Unit, 게이트 순환 유닛)**

: LSTM의 장기의존성 문제 해결책을 유지하며 은닉 상태를 업데이트하는 계산을 줄임 (LSTM 과 유사하면서 복잡한 구조를 간단화)

![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2F808e9894-a70e-4932-b0b8-8325ee8cb7d1%2Fimage.png)

- 2개의 게이트
- LSTM에 있던 **cell-state가 사라짐**(cell-state벡터와 hidden-state 벡터가 하나의 벡터로 통일)
- **하나의 게이트 \*z\**t\*가 forget gate, input gate 모두 제어**
  ex) *z**t*​가 1이면 forget gate 열리고, input 게이트 닫힘
  *z**t*​가 0이면 forget gate 닫히고 ,input 게이트 열림
  \- **Output gate 사라짐**
  (전체 상태 벡터 h가 각 t에서 출력되고 t-1시점 h의 어느 부분이 출력될지를 제어하는 **gate \*r\**t\*​ 가 추가** )

## Attention

### RNN 기반(LSTM,GRU) 모델의 단점

- RNN의 가장 큰 단점을 **기울기 소실로 장기의존성 문제**
  (문장이 길어질수록 앞 단어의 정보 잃어버리는 현상)
  **이 문제를 해결하기 위해 셀 구조를 개선하여 LSTM과 GRU 나옴**

  

- 하지만, 이 모델은 **고정 길이의 hidden-state 벡터**에 모든 단어의 의미를 담아야 함
  (LSTM, GRU가 장기 의존성 문제는 개선했지만 30~50 단어가 넘어가는 단어를
  고정 hidden-state 벡터에 담기 어려움)

  

### Attention

- Attention은 **각 인코더의 시점 마다 생성되는 hidden-state 벡터**를 각각 간직
- 모두 입력된 후 생성된 hidden-state를 디코더로 한 번에 넘겨줌
- 디코더가 인코더에 입력되는 모든 단어의 정보를 활용하여 **장기 의존성 문제를 해결**
  (RNN처럼 최종 hidden-state가 아닌 각 시점의 Hidden-state를 다 받기 때문)![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2F01f5b20b-235e-4b0a-9ebd-c6f2ab5d074f%2Fimage.png)

> **Attention 함수
> 수식 ---> Attention(Q, K, V)**
> \- **쿼리, 키, 밸류**의 개념 등장
> \- attention 함수는 주어진 쿼리(원하는 값)에 대해 모든 key(관련 모든 단어)와의 유사도를 각각 구함
>
> \1) **Q(query)** : 구하고자 하는 대상 == > **현재 시점(t)의 디코더 셀의 hidden state**
> \2) **K(key)** : query와 관련 깊은 단어 ==> **모든 시점의 인코더 셀의 hidden state**
> \3) **V(value)**: key에 대한 의미 ==> **모든 시점의 인코더 셀의 hidden state**

**Attention 매커니즘**

![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2F4fb2203a-ff53-4f3d-ad0a-fb62d10aae38%2Fimage.png)

**i am a student --(번역)---> je suis etudiant** 할 때,
위 그림은 디코더의 세번째 LTSM셀이 출력단어를 예측할 때 attention 매커니즘을 사용하는 과정이다(**주황색 인코더, 초록색 디코더**)

- 세번째 LSTM 셀이 suis를 통해 번역된 단어(etudiant)를 예측하려면 인코더의 **모든 시점의 입력 정보를 참고해야함**

**<순서 정리>**

\1) 디코더의 hidden-state(qeury), 인코더에서 넘어온 hidden-state(key) 벡터를 준비
\2) 각 벡터를 내적
\3) 2)의 값에 softmax 함수 적용
\4) softmax 로 나온 값에 인코더에서 넘어온 value hidden-state 벡터를 곱해줌
\5) 4)번의 벡터를 모두 더한 값과 디코더의 hidden-state 벡터를 통해 출력 단어 결정

**Attention 스코어 시각화**

**번역 : Je suis etudiant => I am a student**
![img](https://velog.velcdn.com/images%2Fqksekf%2Fpost%2F213312fd-83f0-4eea-be6a-1c0eb986aee7%2Fimage.png)